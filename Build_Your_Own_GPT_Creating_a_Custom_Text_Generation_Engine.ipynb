{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyOOlP0Rkyrg2RTV6ynIT8mH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jayasurya227/Build-Your-Own-GPT-Creating-a-Custom-Text-Generation-Engine./blob/main/Build_Your_Own_GPT_Creating_a_Custom_Text_Generation_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Google Drive Mount**"
      ],
      "metadata": {
        "id": "3JqfchKs8jKP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZzxTX9v5xJ8",
        "outputId": "a255cca4-83c4-4872-d0b6-ad2e95c25c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Library Installation And Data Loading**"
      ],
      "metadata": {
        "id": "zmE8qSal8ujN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)"
      ],
      "metadata": {
        "id": "xdTPipSw6X4p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tiny Stories And Stream DataSet Class**"
      ],
      "metadata": {
        "id": "ZBV4YpFF89qQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "class TinyStoriesStreamDataset(IterableDataset):\n",
        "    def __init__(self, dataset_stream, tokenizer, block_size=512, min_length=30):\n",
        "        self.dataset = dataset_stream\n",
        "        self.tokenizer = tokenizer\n",
        "        self.block_size = block_size\n",
        "        self.min_length = min_length\n",
        "\n",
        "    def __iter__(self):\n",
        "        for sample in self.dataset:\n",
        "            text = sample[\"text\"].strip()\n",
        "            if len(text) < self.min_length:\n",
        "                continue\n",
        "\n",
        "            text = re.sub(r'\\s+', ' ', text)\n",
        "            text = re.sub(r'[“”]', '\"', text)\n",
        "            text = re.sub(r\"[‘’]\", \"'\", text)\n",
        "            text = re.sub(r'[^a-zA-Z0-9.,!?\\'\"\\s]', '', text)\n",
        "\n",
        "            tokenized = self.tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                add_special_tokens=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.block_size,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            input_ids = tokenized[\"input_ids\"][0]\n",
        "            attention_mask = tokenized[\"attention_mask\"][0]\n",
        "\n",
        "            yield {\n",
        "                \"input_ids\": input_ids[:-1],\n",
        "                \"labels\": input_ids[1:],\n",
        "                \"attention_mask\": attention_mask[:-1]\n",
        "            }"
      ],
      "metadata": {
        "id": "qzElOUM-6oOi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Tokenizer,Data Loader,Model And Optimizer Setup**"
      ],
      "metadata": {
        "id": "GBuYpLVJ9ZFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "\n",
        "total_samples = 2119719\n",
        "batch_size = 52\n",
        "max_batches_per_epoch = total_samples // batch_size\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "stream_dataset = TinyStoriesStreamDataset(dataset, tokenizer)\n",
        "train_loader = DataLoader(stream_dataset, batch_size=batch_size)\n",
        "\n",
        "config = GPT2Config(\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_positions=512,\n",
        "    n_ctx=512,\n",
        "    n_embd=256,\n",
        "    n_layer=4,\n",
        "    n_head=4,\n",
        "    pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "\n",
        "model = GPT2LMHeadModel(config)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
        "    model = torch.nn.DataParallel(model)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "fcxyBj2T6qCD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop And CheckPoint And Sampling**"
      ],
      "metadata": {
        "id": "fzMr45BQAWjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# Define checkpoint directory\n",
        "checkpoint_dir = Path(\"/content/drive/MyDrive/TinyLLM/model/\")\n",
        "\n",
        "epochs = 10\n",
        "history = []\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for i, batch in enumerate(tqdm(train_loader, total=max_batches_per_epoch)):\n",
        "        if i >= max_batches_per_epoch:\n",
        "            break\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / max_batches_per_epoch\n",
        "    history.append(avg_loss)\n",
        "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save model after every epoch\n",
        "    epoch_checkpoint = checkpoint_dir / f\"tinygpt2_epoch{epoch+1}\"\n",
        "    epoch_checkpoint.mkdir(parents=True, exist_ok=True)\n",
        "    model.save_pretrained(epoch_checkpoint)\n",
        "    tokenizer.save_pretrained(epoch_checkpoint)\n",
        "    print(f\"Model checkpoint saved at {epoch_checkpoint}\")\n",
        "\n",
        "    # Generate sample output\n",
        "    model.eval()\n",
        "    sample_input = tokenizer.encode(\"Once upon a time\", return_tensors=\"pt\").to(device)\n",
        "    generated_ids = model.generate(\n",
        "        sample_input,\n",
        "        max_length=50,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    print(f\"Sample Output:\\n{generated_text}\")\n",
        "    model.train()\n",
        "\n",
        "history_path = Path(\"/content/drive/MyDrive/TinyLLM/training_history.json\")\n",
        "with open(history_path, \"w\") as f:\n",
        "    json.dump(history, f)\n",
        "print(f\"\\nTraining history saved to {history_path}\")"
      ],
      "metadata": {
        "id": "7TfswYtfEBmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resume Training And Check Point**"
      ],
      "metadata": {
        "id": "mDgnt2hpERDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from transformers import GPT2Tokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer from checkpoint (epoch 6)\n",
        "checkpoint_path = Path(\"/content/drive/MyDrive/TinyLLM/model/tinygpt2_epoch6\")\n",
        "model = GPT2LMHeadModel.from_pretrained(checkpoint_path)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(checkpoint_path)\n",
        "\n",
        "total_samples = 2119719\n",
        "batch_size = 52\n",
        "max_batches_per_epoch = total_samples // batch_size\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
        "    model = torch.nn.DataParallel(model)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training parameters\n",
        "checkpoint_dir = Path(\"/content/drive/MyDrive/TinyLLM/model/\")\n",
        "epochs = 12  # Continue up to epoch 10\n",
        "start_epoch = 6  # Start from epoch 6\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(start_epoch, epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for i, batch in enumerate(tqdm(train_loader, total=max_batches_per_epoch)):\n",
        "        if i >= max_batches_per_epoch:\n",
        "            break\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / max_batches_per_epoch\n",
        "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save model after each epoch\n",
        "    epoch_checkpoint = checkpoint_dir / f\"tinygpt2_epoch{epoch+1}\"\n",
        "    epoch_checkpoint.mkdir(parents=True, exist_ok=True)\n",
        "    model.save_pretrained(epoch_checkpoint)\n",
        "    tokenizer.save_pretrained(epoch_checkpoint)\n",
        "    print(f\"Model checkpoint saved at {epoch_checkpoint}\")\n",
        "\n",
        "    # Generate sample output\n",
        "    model.eval()\n",
        "    sample_input = tokenizer.encode(\"Once upon a time\", return_tensors=\"pt\").to(device)\n",
        "    generated_ids = model.generate(\n",
        "        sample_input,\n",
        "        max_length=50,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    print(f\"Sample Output:\\n{generated_text}\")\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "829FhZXqEPkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ganarate Text From  a Saved GPT-2 Check Point**"
      ],
      "metadata": {
        "id": "1Qv0yFEjE33S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "model_directory = \"epoch_5\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_directory)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_directory)\n",
        "\n",
        "\n",
        "def generate(input_text, max_len):\n",
        "\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "  inputs = tokenizer(\n",
        "      input_text,\n",
        "      return_tensors='pt',\n",
        "      padding=True,\n",
        "      return_attention_mask=True\n",
        "  )\n",
        "\n",
        "  output = model.generate(\n",
        "      input_ids=inputs['input_ids'],\n",
        "      attention_mask=inputs['attention_mask'],\n",
        "      max_length=max_len\n",
        "  )\n",
        "\n",
        "  generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  return generated_text\n",
        "\n",
        "print(generate(\"Once there was little boy\",30))\n",
        "print(generate(\"Once there was little girl\",30))\n",
        "print(generate(\"Once there was a cute\",30))\n",
        "print(generate(\"Once there was a cute little\",30))\n",
        "print(generate(\"Once there was a handsome\",30))"
      ],
      "metadata": {
        "id": "03wDOYd8Exa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inferance With PreTrained Tiny Stories Model**"
      ],
      "metadata": {
        "id": "4kYDyApWFKTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-3M')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
        "\n",
        "prompt = \"Once upon a time there was\"\n",
        "\n",
        "\n",
        "def generate(input_text, max_len):\n",
        "\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "  inputs = tokenizer(\n",
        "      input_text,\n",
        "      return_tensors='pt',\n",
        "      padding=True,\n",
        "      return_attention_mask=True\n",
        "  )\n",
        "\n",
        "  output = model.generate(\n",
        "      input_ids=inputs['input_ids'],\n",
        "      attention_mask=inputs['attention_mask'],\n",
        "      max_length=max_len\n",
        "  )\n",
        "\n",
        "  generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "  return generated_text\n",
        "\n",
        "  return output_text\n",
        "\n",
        "print(generate(\"Once there was little boy\",30))\n",
        "print(generate(\"Once there was little girl\",30))\n",
        "print(generate(\"Once there was a cute\",30))\n",
        "print(generate(\"Once there was a cute little\",30))\n",
        "print(generate(\"Once there was a handsome\",30))"
      ],
      "metadata": {
        "id": "bt6DlIjJFVvS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}